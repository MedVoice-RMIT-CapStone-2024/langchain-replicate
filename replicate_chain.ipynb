{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magics to auto-reload external modules in case you are making changes to langchain while working on this notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a token: https://replicate.com/account\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "REPLICATE_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain\n",
    "%pip install -q langchain-community\n",
    "%pip install -q langchain-core\n",
    "%pip install -q replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import Replicate\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AT\\Desktop\\Capstone_ML\\rep\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{ \"Id\": 1, \"age\": 50, \"diagnosis\": \"Type 2 Diabetes\", \"gender\": \"Male\", \"name\": \"Mr. Anderson\" } '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    \"prompt\": \"Send a email from andreas86@telia.se to myfriend@telia.se where you discuss the weather. In the body, describe the current weather in Stockholm as detailed as possible.\\n\\nRespond with json that adheres to the following jsonschema:\\n\\n{jsonschema}\",\n",
    "    \"grammar\": \"\",\n",
    "    \"jsonschema\": \"{\\n \\\"type\\\": \\\"object\\\", \\n \\\"properties\\\": {\\n \\\"Id\\\" : {  \\\"type\\\" : \\\"integer\\\"}, \\n \\\"name\\\" : {\\n \\\"type\\\" : \\\"string\\\"}, \\n \\\"age\\\" : {\\n \\\"type\\\" : \\\"integer\\\"}, \\n \\\"gender\\\" : {\\n \\\"type\\\" : \\\"string\\\" }, \\n \\\"diagnosis\\\" : {\\n \\\"type\\\" : \\\"string\\\" }} }\"\n",
    "}\n",
    "\n",
    "llm = Replicate(\n",
    "    model=\"andreasjansson/llama-2-13b-chat-gguf:60ec5dda9ff9ee0b6f786c9d1157842e6ab3cc931139ad98fe99e08a35c5d4d4\",\n",
    "    model_kwargs={\"temperature\": 0.75, \"max_length\": 500, \"top_p\": 1, \"jsonschema\": \"{\\n \\\"type\\\": \\\"object\\\", \\n \\\"properties\\\": {\\n \\\"Id\\\" : {  \\\"type\\\" : \\\"integer\\\"}, \\n \\\"name\\\" : {\\n \\\"type\\\" : \\\"string\\\"}, \\n \\\"age\\\" : {\\n \\\"type\\\" : \\\"integer\\\"}, \\n \\\"gender\\\" : {\\n \\\"type\\\" : \\\"string\\\" }, \\n \\\"diagnosis\\\" : {\\n \\\"type\\\" : \\\"string\\\" }} }\"},\n",
    ")\n",
    "prompt = \"\"\"\n",
    "System: You are an AI that summarizes medical conversations into a structured JSON format. Given the medical transcript below, provide a summary by extracting key-value pairs. Only use the information explicitly mentioned in the transcript, and you must not infer or assume any details that are not directly stated.\n",
    "Speaker 1: Good morning, Nurse. Could you please update me on the status of our patient, Mr. Anderson?\n",
    "Speaker 2: Good morning, Doctor. Mr. Anderson's blood pressure has stabilized at 100/70. His blood glucose level this morning was 8 mmol/L.\n",
    "Speaker 1: That's good to hear. Has he taken his medication for the morning?\n",
    "Speaker 2: Yes, Doctor. He took his Metformin at 7 AM, as prescribed.\n",
    "Speaker 1: Excellent. Please continue to monitor his vitals and let me know if there are any significant changes.\n",
    "Speaker 2: Absolutely, Doctor. I'll keep you updated.\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"Id\": 1, \"age\": 20, \"diagnosis\": \"headache\", \"gender\": \"male\" ,\"name\": \"Alex\" }"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Replicate(\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    model=\"andreasjansson/llama-2-13b-chat-gguf:60ec5dda9ff9ee0b6f786c9d1157842e6ab3cc931139ad98fe99e08a35c5d4d4\",\n",
    "    model_kwargs={\"temperature\": 0.75, \"max_length\": 500, \"top_p\": 1, \"jsonschema\": \"{\\n \\\"type\\\": \\\"object\\\", \\n \\\"properties\\\": {\\n \\\"Id\\\" : {  \\\"type\\\" : \\\"integer\\\"}, \\n \\\"name\\\" : {\\n \\\"type\\\" : \\\"string\\\"}, \\n \\\"age\\\" : {\\n \\\"type\\\" : \\\"integer\\\"}, \\n \\\"gender\\\" : {\\n \\\"type\\\" : \\\"string\\\" }, \\n \\\"diagnosis\\\" : {\\n \\\"type\\\" : \\\"string\\\" }} }\"},\n",
    ")\n",
    "prompt = \"\"\"\n",
    "User: A male patient Alex is 20 years old have headache. Tell me what is the patient name, age, gender, and diagnosis\n",
    "Assistant:\n",
    "\"\"\"\n",
    "_ = llm.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
